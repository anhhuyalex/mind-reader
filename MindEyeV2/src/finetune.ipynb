{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a8a32a-c77f-4890-b4ee-a87b54dc5526",
   "metadata": {},
   "source": [
    "This script follows MindEyeV1 training procedure (e.g., training a diffusion prior and reconstructing with Versatile Diffusion) except that it uses the newer data loading procedure being used for MindEyeV2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-12 22:02:10,874] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import webdataset as wds\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "# from deepspeed import DeepSpeedEngine\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5d2e32-6027-4a19-bef4-5ca068db35bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "# ## UNCOMMENT BELOW SECTION AND COMMENT OUT DEEPSPEED SECTION TO AVOID USING DEEPSPEED ###\n",
    "use_deepspeed = False\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\") # ['no', 'fp8', 'fp16', 'bf16']\n",
    "global_batch_size = 28\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "\n",
    "### DEEPSPEED INITIALIZATION ###\n",
    "# use_deepspeed = True\n",
    "# import deepspeed\n",
    "# if num_devices <= 1 and utils.is_interactive():\n",
    "#     global_batch_size = batch_size = 28\n",
    "#     print(f\"Setting batch_size to {batch_size}\")\n",
    "#     # can emulate a distributed environment for deepspeed to work in jupyter notebook\n",
    "#     os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "#     os.environ[\"MASTER_PORT\"] = str(np.random.randint(10000)+9000)\n",
    "#     os.environ[\"RANK\"] = \"0\"\n",
    "#     os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "#     os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "#     os.environ[\"GLOBAL_BATCH_SIZE\"] = str(global_batch_size) # set this to your batch size!\n",
    "# else:\n",
    "#     global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]    \n",
    "#     batch_size = int(os.environ[\"GLOBAL_BATCH_SIZE\"]) // num_devices\n",
    "#     if num_devices <= 1:\n",
    "#         os.environ[\"RANK\"] = \"0\"\n",
    "#         os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "#         os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "# # alter the deepspeed config according to your global and local batch size\n",
    "# if local_rank == 0:\n",
    "#     with open('deepspeed_config_stage2_cpuoffload.json', 'r') as file:\n",
    "#         config = json.load(file)\n",
    "#     config['train_batch_size'] = int(os.environ[\"GLOBAL_BATCH_SIZE\"])\n",
    "#     config['train_micro_batch_size_per_gpu'] = batch_size\n",
    "#     config['bf16'] = {'enabled': False}\n",
    "#     config['fp16'] = {'enabled': True}\n",
    "#     with open('deepspeed_config_stage2_cpuoffload.json', 'w') as file:\n",
    "#         json.dump(config, file)\n",
    "# else:\n",
    "#     # give some time for the local_rank=0 gpu to prep new deepspeed config file\n",
    "#     time.sleep(10)\n",
    "# deepspeed_plugin = DeepSpeedPlugin(\"deepspeed_config_stage2_cpuoffload.json\")\n",
    "# accelerator = Accelerator(split_batches=False, deepspeed_plugin=deepspeed_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 1629172\n",
      "device: cuda\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "if not use_deepspeed: \n",
    "    batch_size = global_batch_size // num_devices\n",
    "\n",
    "# set data_type to match your mixed precision (automatically set based on deepspeed config)\n",
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    data_type = torch.bfloat16\n",
    "elif accelerator.mixed_precision == \"fp16\":\n",
    "    data_type = torch.float16\n",
    "else:\n",
    "    data_type = torch.float32\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc428a-68c2-4781-91c7-5b17385af9aa",
   "metadata": {},
   "source": [
    "## Training model on just 1 subject (Baseline)\n",
    "Set both \"subj\" and \"subj_list\" to the subject you want to train on.\n",
    "You can set \"num_sessions\" to the number of sessions of training data to include for the given subject. Setting it to -1 means using all possible sessions.\n",
    "\n",
    "## Training model on all subjects except 1 (Stage 1)\n",
    "Do not set \"subj_list\" and let it be set to default.\n",
    "Set \"subj\" to the subject you *don't* want to train on. Model will train across all the other subjects.\n",
    "\n",
    "## Fine-tuning a pre-trained model on a single subject (Stage 2)\n",
    "Set both \"subj\" and \"subj_list\" to the subject you want to fine-tune on.\n",
    "Set \"stage2\" and \"resume_from_ckpt\" to True and specify \"model_name\" as the same model_name used for stage 1. Should be the name of folder containing pre-trained checkpoint in \"train_logs\".\n",
    "You can set \"num_sessions\" to the number of sessions of training data to include for the given subject. Setting it to -1 means using all possible sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: stage2_noprior_subj01x\n",
      "--data_path=/fsx/proj-fmri/shared/mindeyev2_dataset                     --seed=42                    --model_name=stage2_noprior_subj01x --no-use_prior                     --subj=1 --subj_list=1 --batch_size=28                     --num_samples=6999 --samples_pool=7000                    --max_lr=3e-5 --mixup_pct=.33 --num_epochs=20 --no-ckpt_saving --stage2                     --resume_from_ckpt=../train_logs/stage1_prior_subj01_h2048_3e5                     --filter_samples=./cache/subj01_permuted_samples.txt\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"stage2_noprior_subj01x\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "    \n",
    "    # # Stage 1\n",
    "    # jupyter_args = f\"--data_path=/weka/proj-fmri/shared/mindeyev2_dataset \\\n",
    "    #                 --model_name={model_name} --no-use_prior \\\n",
    "    #                 --subj=1 --batch_size={batch_size} --num_sessions=-1 \\\n",
    "    #                 --max_lr=3e-5 --mixup_pct=.33 --num_epochs=40 --no-ckpt_saving\"\n",
    "    \n",
    "    # # Stage 2\n",
    "    jupyter_args = f\"--data_path=/weka/proj-fmri/shared/mindeyev2_dataset \\\n",
    "                    --seed=42\\\n",
    "                    --model_name={model_name} --no-use_prior \\\n",
    "                    --subj=1 --subj_list=1 --batch_size={batch_size} \\\n",
    "                    --max_lr=3e-5 --mixup_pct=.33 --num_epochs=20 --no-ckpt_saving --stage2 \\\n",
    "                    --resume_from_ckpt=../train_logs/stage1_prior_subj01_h2048_3e5 \\\n",
    "                    --filter_samples=./cache/subj01_permuted_samples.txt\"\n",
    "    \n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d264fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions -1\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/weka/proj-fmri/shared/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=-1,\n",
    "    help=\"Number of training sessions to include (-1 = all possible sessions)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_samples\", type=int, default=-1, \n",
    "    help=\"Number of samples to filter from training\")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=32,\n",
    "    help=\"Batch size can be increased by 10x if only training v2c and not diffusion diffuser\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--filter_samples\", type=str, default=None,\n",
    "    help=\"file path that contains samples to filter\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--val_filter_samples\", type=str, default=None,\n",
    "    help=\"file path that contains samples from train_url to filter and use for validation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--when_to_preload\", type=str, default=\"before_each_epoch\",\n",
    "    choices=[\"before_training\",\"before_each_epoch\"],\n",
    "    help=\"when to preload the dataset (before training or before each epoch)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_group_name\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",type=str,default=None,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stage2\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"fine-tuning from a pre-trained model trained across subjects?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_mult\",type=float,default=3,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj_list\", type=int, nargs='+', default=[1,2,3,4,5,6,7,8],\n",
    "    help=\"number of subjects\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-5,\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\",type=str,default='/weka/proj-fmri/shared/cache',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--offline_cache_dir\",type=str,default='./activelearning_cache',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=None,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "if args.seed is None:\n",
    "    args.seed = time.time()\n",
    "if len(subj_list)>1:\n",
    "    subj_list.remove(subj)\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)\n",
    "\n",
    "if wandb_log and local_rank==0:\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cd7f2c-37fd-426b-a0c6-633e51bc4c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outdir = os.path.abspath(f'./train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "if use_image_aug:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), (0.6,1), p=0.3),\n",
    "        kornia.augmentation.Resize((224, 224)),\n",
    "        kornia.augmentation.RandomHorizontalFlip(p=0.3),\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.3),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "source": [
    "## Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "setting batch_size to same as stage 1...\n",
      "batch_size = 4 num_iterations_per_epoch = 1749 num_samples_per_epoch = 6999\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "\n",
    "nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "# if num_sessions == 0: num_sessions = nsessions_allsubj[s-1]\n",
    "num_samples_per_epoch = 30000 // num_devices \n",
    "\n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "if stage2 and len(subj_list)==1: # dividing batch size by 7 to retain same batch size as stage 1, which divides across subj_list\n",
    "    batch_size = batch_size // 7\n",
    "    print(\"setting batch_size to same as stage 1...\")\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81084834-035f-4465-ad59-59e6b806a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj01 training with /fsx/proj-fmri/shared/mindeyev2_dataset/wds/subj01/train/{0..39}.tar train_url\n",
      "subj01 training with 6999 samples ['sample000024949', 'sample000010112', 'sample000023439', 'sample000024179', 'sample000017962', 'sample000013123', 'sample000026541', 'sample000016797', 'sample000023581', 'sample000017393', 'sample000026196', 'sample000000288', 'sample000013996', 'sample000000223', 'sample000026020', 'sample000001867', 'sample000002154', 'sample000000062', 'sample000011400', 'sample000004974', 'sample000006524', 'sample000021591', 'sample000008263', 'sample000014583', 'sample000019257', 'sample000014868', 'sample000001708', 'sample000026273', 'sample000023982', 'sample000020763', 'sample000002723', 'sample000018251', 'sample000010641', 'sample000015821', 'sample000022258', 'sample000007489', 'sample000009150', 'sample000005141', 'sample000024815', 'sample000016595', 'sample000010784', 'sample000019419', 'sample000016771', 'sample000007865', 'sample000010442', 'sample000001476', 'sample000011864', 'sample000011567', 'sample000021626', 'sample000021919', 'sample000007741', 'sample000000668', 'sample000008116', 'sample000001856', 'sample000018401', 'sample000018664', 'sample000021223', 'sample000001380', 'sample000023452', 'sample000013619', 'sample000007303', 'sample000013730', 'sample000016739', 'sample000024581', 'sample000003314', 'sample000017789', 'sample000024783', 'sample000000838', 'sample000002458', 'sample000010048', 'sample000021029', 'sample000019321', 'sample000026130', 'sample000014288', 'sample000024482', 'sample000004266', 'sample000019592', 'sample000026890', 'sample000013098', 'sample000008365', 'sample000019574', 'sample000017791', 'sample000000083', 'sample000023169', 'sample000022799', 'sample000005241', 'sample000012038', 'sample000020459', 'sample000010462', 'sample000013232', 'sample000022339', 'sample000018920', 'sample000025851', 'sample000026082', 'sample000007029', 'sample000009367', 'sample000024616', 'sample000020637', 'sample000009043', 'sample000024427'] 6999...\n",
      "num_voxels for subj01: 15724\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "/fsx/proj-fmri/shared/mindeyev2_dataset/wds/subj01/test/0.tar\n",
      "Loaded test dl for subj1! num_test=2770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "val_data = {}\n",
    "val_dl = {}\n",
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for si, s in enumerate(args.subj_list):\n",
    "    if args.num_sessions == -1:\n",
    "        train_url = f\"{args.data_path}/wds/subj0{s}/train/\" + \"{0..\" + f\"{nsessions_allsubj[s-1]-1}\" + \"}.tar\"\n",
    "        print(f\"subj0{args.subj_list[si]} training with {train_url} train_url\")\n",
    "    else:\n",
    "        print(f\"subj0{args.subj_list[si]} training with {args.num_sessions} sessions\")\n",
    "        train_url = f\"{args.data_path}/wds/subj0{s}/train/\" + \"{0..\" + f\"{args.num_sessions-1}\" + \"}.tar\"\n",
    "    \n",
    "    if args.filter_samples is not None:\n",
    "        \n",
    "        filter_samples_list = np.load(args.filter_samples)\n",
    "        print (\"filter_samples_list\", len(filter_samples_list))\n",
    "        def filter_samples_func(sample):\n",
    "            if sample[\"behav.npy\"][0,0] not in filter_samples_list:\n",
    "                return None\n",
    "            return sample\n",
    "        \n",
    "    else:\n",
    "        filter_samples_list = None\n",
    "        def filter_samples_func(sample):\n",
    "            return sample\n",
    "        \n",
    "    if args.val_filter_samples is not None:\n",
    "        val_filter_samples_list = np.load(args.val_filter_samples)\n",
    "        print (\"val_filter_samples_list\", len(val_filter_samples_list))\n",
    "        def val_filter_samples_func(sample):\n",
    "            if sample[\"behav.npy\"][0,0] not in val_filter_samples_list:\n",
    "                return None\n",
    "            return sample\n",
    "    else:\n",
    "        val_filter_samples_list = None\n",
    "        def val_filter_samples_func(sample):\n",
    "            return sample\n",
    "    # make training dataset and data loader\n",
    "    train_data[f'subj0{s}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                        .shuffle(750, initial=2500, rng=random.Random(args.seed))\\\n",
    "                        .decode(\"torch\").map(filter_samples_func)\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    train_dl[f'subj0{s}'] = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    " \n",
    "    # make validation dataset and data loader\n",
    "    val_data[f'subj0{s}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                        .shuffle(750, initial=2500, rng=random.Random(args.seed))\\\n",
    "                        .decode(\"torch\").map(val_filter_samples_func)\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    val_dl[f'subj0{s}'] = torch.utils.data.DataLoader(val_data[f'subj0{s}'], batch_size=batch_size if args.val_filter_samples is None else len(val_filter_samples_list),\n",
    "                                                      shuffle=False, drop_last=True, pin_memory=True)\n",
    "    \n",
    "    # Load hdf5 data for betas, but don't put everything into memory\n",
    "    f = h5py.File(f'{args.data_path}/betas_all_subj0{s}_fp32.hdf5', 'r')\n",
    "    \n",
    "    betas = f['betas'][:]\n",
    "    betas = torch.Tensor(betas).to(\"cpu\").to(data_type)\n",
    "    num_voxels_list.append(betas[0].shape[-1])\n",
    "    num_voxels[f'subj0{s}'] = betas[0].shape[-1]\n",
    "    voxels[f'subj0{s}'] = betas\n",
    "    print(f\"num_voxels for subj0{s}: {num_voxels[f'subj0{s}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "\n",
    "# Validate only on the subject from first index of subj_list\n",
    "num_test = [2770,2770,2113,1985,2770,2113,2770,1985] # maximum possible number of test samples per subj\n",
    "test_url = f\"{args.data_path}/wds/subj0{subj_list[0]}/test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test[int(subj_list[0])-1], shuffle=False, drop_last=False, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj_list[0]}! num_test={num_test[int(subj_list[0])-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c13b4b84-094c-4b5b-bace-26c155aa6181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images to cpu! torch.Size([73000, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Preload 73k NSD images\n",
    "f = h5py.File(f'{args.data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:]\n",
    "images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22506e9782774b36aa71f4716d490854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import VersatileDiffusionPipeline, UniPCMultistepScheduler\n",
    "# if you get an error here, make sure your diffusers package is version 0.13.0!\n",
    "# vd_pipe = VersatileDiffusionPipeline.from_pretrained(\"shi-labs/versatile-diffusion\", torch_dtype=data_type, cache_dir=cache_dir)\n",
    "# vd_pipe.scheduler = UniPCMultistepScheduler.from_pretrained(\"shi-labs/versatile-diffusion\", subfolder=\"scheduler\", cache_dir=cache_dir)\n",
    "vd_pipe = VersatileDiffusionPipeline.from_pretrained(\"/weka/proj-fmri/shared/cache/versatile-diffusion\", torch_dtype=data_type)\n",
    "vd_pipe.scheduler = UniPCMultistepScheduler.from_pretrained(\"/weka/proj-fmri/shared/cache/versatile-diffusion\", subfolder=\"scheduler\")\n",
    "vd_pipe.to(device)#(torch.device(f\"cuda:{local_rank}\"))\n",
    "clip_model = vd_pipe.image_encoder\n",
    "clip_model.to(data_type)\n",
    "clip_model.eval()\n",
    "clip_model.requires_grad_(False)\n",
    "clip_seq_dim = 257\n",
    "clip_emb_dim = 768\n",
    "\n",
    "## testing it out:\n",
    "# utils.get_clip_embeddings(clip_model,images[:1].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "## MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "32,204,802 total\n",
      "32,204,802 trainable\n",
      "param counts:\n",
      "32,204,802 total\n",
      "32,204,802 trainable\n",
      "torch.Size([2, 15724]) torch.Size([2, 2048])\n"
     ]
    }
   ],
   "source": [
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "        self.temp = nn.Parameter(torch.Tensor([5.3]))\n",
    "        self.bias = nn.Parameter(torch.Tensor([-2.]))\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = self.linears[subj_idx](x)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test on subject 1 with fake data\n",
    "b = torch.randn((2,num_voxels_list[0]))\n",
    "print(b.shape, model.ridge(b,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3602c333-d029-465c-8fb4-c3ccffdba6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "445,348,864 total\n",
      "445,348,864 trainable\n",
      "param counts:\n",
      "477,553,666 total\n",
      "477,553,666 trainable\n",
      "in torch.Size([2, 2048])\n",
      "out torch.Size([2, 197376]) torch.Size([2, 257, 768])\n"
     ]
    }
   ],
   "source": [
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, out_dim=768, in_dim=15724, h=4096, n_blocks=n_blocks, drop=.15, clip_size=768, use_projector=True):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.n_blocks = n_blocks\n",
    "        self.clip_size = clip_size\n",
    "        self.use_projector = use_projector\n",
    "        \n",
    "        self.mlps = nn.ModuleList([\n",
    "            self.mlp(h, h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.lin1 = nn.Linear(h, out_dim, bias=True)\n",
    "        self.clip_proj = self.projector(clip_size, clip_size)\n",
    "            \n",
    "    def projector(self, in_dim, out_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, out_dim)\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        for res_block in range(self.n_blocks):\n",
    "            x = self.mlps[res_block](x)\n",
    "            x += residual\n",
    "            residual = x\n",
    "        x = x.reshape(len(x), -1)\n",
    "        x = self.lin1(x)\n",
    "        if self.use_projector: return x, self.clip_proj(x.reshape(len(x), -1, self.clip_size))\n",
    "        return x\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) \n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,hidden_dim))\n",
    "print(\"in\",b.shape)\n",
    "backbone_, clip_ = model.backbone(b)\n",
    "print(\"out\",backbone_.shape, clip_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995d46e-884a-47a4-884b-6e0de3825ebd",
   "metadata": {},
   "source": [
    "## Load diffusion prior + Versatile Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f1a5f53-6003-44a9-be14-44dea2393942",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 64\n",
    "    heads = clip_emb_dim//64 # heads * dim_head = 12 * 64 = 768\n",
    "    guidance_scale = 3.5\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = VersatileDiffusionPriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "        voxel2clip=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8950fc44-6a1f-4d69-985c-5a35e194679e",
   "metadata": {},
   "source": [
    "## Setup optimizer and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 34980\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "477,553,666 total\n",
      "477,553,666 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "if use_prior:\n",
    "    opt_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "else:\n",
    "    opt_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):    \n",
    "    if stage2:\n",
    "        tag = tag + \"_stage2\"\n",
    "    if use_deepspeed:\n",
    "        deepspeed.DeepSpeedEngine.save_checkpoint(model, save_dir=outdir, tag=tag)\n",
    "        ckpt_path = outdir+f'/{tag}/{tag}.npy'\n",
    "        np.save(ckpt_path, {\n",
    "            'epoch': epoch,\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs})\n",
    "    else:\n",
    "        ckpt_path = outdir+f'/{tag}.pth'\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "        del unwrapped_model\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "    \n",
    "from collections import OrderedDict\n",
    "def filter_params(odict):\n",
    "    return OrderedDict((k, v) for k, v in odict.items() if (not k.startswith('ridge')) and (not k.startswith('ridge.linears.0')))\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,stage2=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    if use_deepspeed:\n",
    "        state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "        if stage2: state_dict = filter_params(checkpoint['model_state_dict'])\n",
    "        model.load_state_dict(state_dict, strict=strict)\n",
    "        if load_epoch:\n",
    "            np_ckpt = np.load(outdir+f'/{tag}/{tag}.npy', allow_pickle=True).tolist()\n",
    "            globals()[\"epoch\"] = np_ckpt['epoch']\n",
    "            print(\"Epoch\",epoch)\n",
    "    else:\n",
    "        print (f\"loading from {tag}/last.pth\")\n",
    "        checkpoint = torch.load(tag+'/last.pth', map_location='cpu')\n",
    "        if stage2: \n",
    "            state_dict = filter_params(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "        if load_epoch:\n",
    "            globals()[\"epoch\"] = checkpoint['epoch']\n",
    "            print(\"Epoch\",epoch)\n",
    "        if load_optimizer:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if load_lr:\n",
    "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        model.load_state_dict(state_dict, strict=strict)\n",
    "        del checkpoint\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, val_losses, lrs = [], [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce285a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5453c316-0cb0-4bee-8585-f44dff746e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---loading /fsx/proj-fmri/alexnguyen/mind-reader/MindEyeV2/src/train_logs/stage2_noprior_subj01x/../train_logs/stage1_prior_subj01_h2048_3e5.pth ckpt---\n",
      "\n",
      "loading from ../train_logs/stage1_prior_subj01_h2048_3e5/last.pth\n",
      "new model_name: stage2_noprior_subj01x_stage2\n"
     ]
    }
   ],
   "source": [
    "# setup weights and biases (optional)\n",
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    wandb_project = 'eye_int'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    wandb.login(host='https://stability.wandb.io') # need to configure wandb environment beforehand\n",
    "    if args.filter_samples is not None:\n",
    "        wandb_model_name = model_name + f\"{args.filter_samples.split('/')[-1].split('.')[0]}\"\n",
    "    else:\n",
    "        wandb_model_name = model_name\n",
    "    offline_logfile = f\"{wandb_model_name}.log\"\n",
    "    wandb_config = {\n",
    "      \"model_name\": wandb_model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_params\": num_params,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"num_test\": num_test,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "      \"num_sessions\": args.num_sessions,\n",
    "    #   \"samples_list\": np.array2string(filter_samples_list, separator=','),\n",
    "      \"offline_logfile\": f\"{args.offline_cache_dir}/{offline_logfile}\",\n",
    "      \"filter_samples\": args.filter_samples\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)   \n",
    "    print(\"wandb_id:\",wandb_model_name)\n",
    "    wandb.init(\n",
    "        project=wandb_project,\n",
    "        name=wandb_model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "        group=args.wandb_group_name\n",
    "    )\n",
    "    utils.save_file_pickle(f\"{args.offline_cache_dir}/{offline_logfile}\", {\n",
    "            \"log\": \"dataset\",\n",
    "            \"filter_samples\": np.load(args.filter_samples) if args.filter_samples is not None else None,\n",
    "        })\n",
    "else:\n",
    "    wandb_log = False\n",
    "\n",
    "# load saved ckpt model weights into current model\n",
    "if not stage2:\n",
    "    if resume_from_ckpt:\n",
    "        load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True,strict=True)\n",
    "    elif wandb_log:\n",
    "        if wandb.run.resumed:\n",
    "            load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True,strict=True)\n",
    "else:\n",
    "    assert resume_from_ckpt is not None \n",
    "    if resume_from_ckpt:\n",
    "        load_ckpt(resume_from_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,stage2=True)\n",
    "    # if wandb_log:\n",
    "    #     if wandb.run.resumed:\n",
    "    #         load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True,strict=True,stage2=False)\n",
    "        \n",
    "    model_name = model_name + \"_stage2\"\n",
    "    print(\"new model_name:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "model, optimizer, *train_dls, lr_scheduler = accelerator.prepare(model, optimizer, *train_dls, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage2_noprior_subj01x_stage2 starting with epoch 0 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_clip 1.7496360540390015\n",
      "loss_clip 2.1375937461853027\n",
      "loss_clip 1.2709734439849854\n",
      "loss_clip 2.8731210231781006\n",
      "loss_clip 1.0438885688781738\n",
      "loss_clip 1.5937011241912842\n",
      "loss_clip 2.0841445922851562\n",
      "loss_clip 0.6373376846313477\n",
      "loss_clip 2.977015495300293\n",
      "loss_clip 1.4465317726135254\n",
      "loss_clip 2.9156553745269775\n",
      "loss_clip 0.9006137251853943\n",
      "loss_clip 2.2193243503570557\n",
      "loss_clip 0.8523720502853394\n",
      "loss_clip 1.4049808979034424\n",
      "loss_clip 2.3512558937072754\n",
      "loss_clip 1.6752599477767944\n",
      "loss_clip 1.0283102989196777\n",
      "loss_clip 0.8787903785705566\n",
      "loss_clip 1.0431147813796997\n",
      "loss_clip 2.319370746612549\n",
      "loss_clip 3.09188175201416\n",
      "loss_clip 1.673975944519043\n",
      "loss_clip 1.4254192113876343\n",
      "loss_clip 1.2410231828689575\n",
      "loss_clip 2.4611940383911133\n",
      "loss_clip 2.134625196456909\n",
      "loss_clip 2.615417957305908\n",
      "loss_clip 2.354874610900879\n",
      "loss_clip 2.612333297729492\n",
      "loss_clip 2.295623779296875\n",
      "loss_clip 1.838867425918579\n",
      "loss_clip 0.49842822551727295\n",
      "loss_clip 0.813591480255127\n",
      "loss_clip 2.176910638809204\n",
      "loss_clip 2.8100457191467285\n",
      "loss_clip 2.542389392852783\n",
      "loss_clip 2.7118241786956787\n",
      "loss_clip 2.235377788543701\n",
      "loss_clip 1.3916032314300537\n",
      "loss_clip 1.2039239406585693\n",
      "loss_clip 3.959501266479492\n",
      "loss_clip 2.2958359718322754\n",
      "loss_clip 1.6576348543167114\n",
      "loss_clip 2.6237454414367676\n",
      "loss_clip 2.7416505813598633\n",
      "loss_clip 2.9403843879699707\n",
      "loss_clip 2.234757423400879\n",
      "loss_clip 1.8625657558441162\n",
      "loss_clip 1.1704871654510498\n",
      "loss_clip 2.3038201332092285\n",
      "loss_clip 2.3163113594055176\n",
      "loss_clip 3.3495535850524902\n",
      "loss_clip 0.6964093446731567\n",
      "loss_clip 2.4660685062408447\n",
      "loss_clip 2.84274959564209\n",
      "loss_clip 1.6645517349243164\n",
      "loss_clip 1.8551881313323975\n",
      "loss_clip 1.4068477153778076\n",
      "loss_clip 3.1603753566741943\n",
      "loss_clip 0.5943812131881714\n",
      "loss_clip 2.4516420364379883\n",
      "loss_clip 1.445655345916748\n",
      "loss_clip 0.8958901166915894\n",
      "loss_clip 0.5921451449394226\n",
      "loss_clip 2.3385872840881348\n",
      "loss_clip 2.1683590412139893\n",
      "loss_clip 1.49930739402771\n",
      "loss_clip 1.5541325807571411\n",
      "loss_clip 1.4271810054779053\n",
      "loss_clip 2.2289323806762695\n",
      "loss_clip 0.7740148305892944\n",
      "loss_clip 0.7354207038879395\n",
      "loss_clip 1.9770002365112305\n",
      "loss_clip 2.6029415130615234\n",
      "loss_clip 1.8366469144821167\n",
      "loss_clip 0.47293609380722046\n",
      "loss_clip 1.726993203163147\n",
      "loss_clip 3.3671302795410156\n",
      "loss_clip 1.8341810703277588\n",
      "loss_clip 2.0164923667907715\n",
      "loss_clip 1.0495333671569824\n",
      "loss_clip 1.8312013149261475\n",
      "loss_clip 1.7566534280776978\n",
      "loss_clip 1.7861485481262207\n",
      "loss_clip 1.7750520706176758\n",
      "loss_clip 2.0451338291168213\n",
      "loss_clip 1.0758740901947021\n",
      "loss_clip 2.969676971435547\n",
      "loss_clip 1.2377421855926514\n",
      "loss_clip 1.7725162506103516\n",
      "loss_clip 1.2614754438400269\n",
      "loss_clip 1.9493215084075928\n",
      "loss_clip 1.5974154472351074\n",
      "loss_clip 1.0800727605819702\n",
      "loss_clip 1.0795793533325195\n",
      "loss_clip 2.405224323272705\n",
      "loss_clip 1.4704538583755493\n",
      "loss_clip 1.4341294765472412\n",
      "loss_clip 1.68394136428833\n",
      "loss_clip 1.3725292682647705\n",
      "loss_clip 2.542741537094116\n",
      "loss_clip 2.2944531440734863\n",
      "loss_clip 2.09940767288208\n",
      "loss_clip 1.7424719333648682\n",
      "loss_clip 0.6645459532737732\n",
      "loss_clip 1.4878480434417725\n",
      "loss_clip 1.0211142301559448\n",
      "loss_clip 0.5825492143630981\n",
      "loss_clip 1.5259824991226196\n",
      "loss_clip 1.047891616821289\n",
      "loss_clip 1.5569247007369995\n",
      "loss_clip 2.241703748703003\n",
      "loss_clip 0.6379823684692383\n",
      "loss_clip 2.1427035331726074\n",
      "loss_clip 1.2649811506271362\n",
      "loss_clip 1.0438721179962158\n",
      "loss_clip 2.0209360122680664\n",
      "loss_clip 1.638336420059204\n",
      "loss_clip 3.4122133255004883\n",
      "loss_clip 0.10064627230167389\n",
      "loss_clip 1.469896912574768\n",
      "loss_clip 2.0535097122192383\n",
      "loss_clip 1.5019099712371826\n",
      "loss_clip -0.0\n",
      "loss_clip 0.937774658203125\n",
      "loss_clip 0.7975133657455444\n",
      "loss_clip 2.2389705181121826\n",
      "loss_clip 0.6505962610244751\n",
      "loss_clip 1.9610252380371094\n",
      "loss_clip 2.302899122238159\n",
      "loss_clip 1.4681358337402344\n",
      "loss_clip 0.9228907823562622\n",
      "loss_clip 1.0577521324157715\n",
      "loss_clip 1.333573579788208\n",
      "loss_clip 1.190319299697876\n",
      "loss_clip 1.4798110723495483\n",
      "loss_clip 1.3635411262512207\n",
      "loss_clip 0.09774773567914963\n",
      "loss_clip 1.7083660364151\n",
      "loss_clip 0.43077027797698975\n",
      "loss_clip 0.9666932225227356\n",
      "loss_clip 1.908052682876587\n",
      "loss_clip 0.6248775720596313\n",
      "loss_clip 1.2249410152435303\n",
      "loss_clip 0.7880018949508667\n",
      "loss_clip 1.0715374946594238\n",
      "loss_clip 0.2415325939655304\n",
      "loss_clip 1.474402666091919\n",
      "loss_clip 1.8198137283325195\n",
      "loss_clip 1.6456096172332764\n",
      "loss_clip 2.0517964363098145\n",
      "loss_clip 0.38063472509384155\n",
      "loss_clip 0.9246237277984619\n",
      "loss_clip 1.5588669776916504\n",
      "loss_clip 1.1067883968353271\n",
      "loss_clip 1.578076958656311\n",
      "loss_clip 0.3981400430202484\n",
      "loss_clip 1.2414673566818237\n",
      "loss_clip 1.1454274654388428\n",
      "loss_clip 0.3114193081855774\n",
      "loss_clip 0.8665561676025391\n",
      "loss_clip 0.7477943897247314\n",
      "loss_clip 1.886286973953247\n",
      "loss_clip 2.2744994163513184\n",
      "loss_clip 1.6816941499710083\n",
      "loss_clip 0.3078378438949585\n",
      "loss_clip 0.03349324315786362\n",
      "loss_clip 1.33150315284729\n",
      "loss_clip 0.7297360897064209\n",
      "loss_clip 1.3257019519805908\n",
      "loss_clip 0.6711329221725464\n",
      "loss_clip 1.2960543632507324\n",
      "loss_clip 1.8772828578948975\n",
      "loss_clip 2.135490894317627\n",
      "loss_clip 1.8573191165924072\n",
      "loss_clip 0.57521653175354\n",
      "loss_clip 2.737523078918457\n",
      "loss_clip 1.1272850036621094\n",
      "loss_clip 1.9203230142593384\n",
      "loss_clip 0.5287619829177856\n",
      "loss_clip 0.4823382794857025\n",
      "loss_clip 1.0527081489562988\n",
      "loss_clip 0.36484840512275696\n",
      "loss_clip 1.4590435028076172\n",
      "loss_clip 0.396633505821228\n",
      "loss_clip 0.65843665599823\n",
      "loss_clip 1.2534161806106567\n",
      "loss_clip 0.708842933177948\n",
      "loss_clip 1.561528205871582\n",
      "loss_clip 2.113192081451416\n",
      "loss_clip 2.084737777709961\n",
      "loss_clip 1.2587087154388428\n",
      "loss_clip 3.433736801147461\n",
      "loss_clip 1.147418737411499\n",
      "loss_clip 0.45770418643951416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 0/20 [00:45<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m image \u001b[38;5;241m=\u001b[39m image_iters[train_i]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_image_aug: \n\u001b[0;32m---> 58\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mimg_augment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m clip_target \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_clip_embeddings(clip_model,image)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(torch\u001b[38;5;241m.\u001b[39misnan(clip_target))\n",
      "File \u001b[0;32m/fsx/proj-fmri/alexnguyen/mind-reader/MindEyeV2/src/mindeye/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fsx/proj-fmri/alexnguyen/mind-reader/MindEyeV2/src/mindeye/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx/proj-fmri/alexnguyen/mind-reader/MindEyeV2/src/mindeye/lib/python3.11/site-packages/kornia/augmentation/container/augment.py:349\u001b[0m, in \u001b[0;36mAugmentationSequential.forward\u001b[0;34m(self, params, data_keys, *args)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    348\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_submodule(param\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 349\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_args\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;66;03m# Make sure we are unpacking a list whilst post-proc\u001b[39;00m\n\u001b[1;32m    354\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m [outputs]\n",
      "File \u001b[0;32m/fsx/proj-fmri/alexnguyen/mind-reader/MindEyeV2/src/mindeye/lib/python3.11/site-packages/kornia/augmentation/container/ops.py:109\u001b[0m, in \u001b[0;36mAugmentationSequentialOps.transform\u001b[0;34m(self, module, param, extra_args, data_keys, *arg)\u001b[0m\n\u001b[1;32m    107\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_op(dcate)\n\u001b[1;32m    108\u001b[0m     extra_arg \u001b[38;5;241m=\u001b[39m extra_args[dcate] \u001b[38;5;28;01mif\u001b[39;00m dcate \u001b[38;5;129;01min\u001b[39;00m extra_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_arg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/fsx/proj-fmri/alexnguyen/mind-reader/MindEyeV2/src/mindeye/lib/python3.11/site-packages/kornia/augmentation/container/ops.py:159\u001b[0m, in \u001b[0;36mInputSequentialOps.transform\u001b[0;34m(cls, input, module, param, extra_args)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, module: Module, param: ParamItem, extra_args: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, (_AugmentationBase, K\u001b[38;5;241m.\u001b[39mMixAugmentationBaseV2)):\n\u001b[0;32m--> 159\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance_module_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, (K\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mImageSequentialBase,)):\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mtransform_inputs(\u001b[38;5;28minput\u001b[39m, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_sequential_module_param(param), extra_args\u001b[38;5;241m=\u001b[39mextra_args)\n",
      "File \u001b[0;32m/fsx/proj-fmri/alexnguyen/mind-reader/MindEyeV2/src/mindeye/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fsx/proj-fmri/alexnguyen/mind-reader/MindEyeV2/src/mindeye/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx/proj-fmri/alexnguyen/mind-reader/MindEyeV2/src/mindeye/lib/python3.11/site-packages/kornia/augmentation/base.py:210\u001b[0m, in \u001b[0;36m_BasicAugmentationBase.forward\u001b[0;34m(self, input, params, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_prob\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tensor([\u001b[38;5;28;01mTrue\u001b[39;00m] \u001b[38;5;241m*\u001b[39m batch_shape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    208\u001b[0m params, flags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_kwargs_to_params_and_flags(params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 210\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_output_tensor(output, input_shape) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeepdim \u001b[38;5;28;01melse\u001b[39;00m output\n",
      "File \u001b[0;32m/fsx/proj-fmri/alexnguyen/mind-reader/MindEyeV2/src/mindeye/lib/python3.11/site-packages/kornia/augmentation/_2d/base.py:125\u001b[0m, in \u001b[0;36mRigidAffineAugmentationBase2D.apply_func\u001b[0;34m(self, in_tensor, params, flags)\u001b[0m\n\u001b[1;32m    122\u001b[0m     flags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags\n\u001b[1;32m    124\u001b[0m trans_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_transformation_matrix(in_tensor, params, flags)\n\u001b[0;32m--> 125\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_matrix \u001b[38;5;241m=\u001b[39m trans_matrix\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/fsx/proj-fmri/alexnguyen/mind-reader/MindEyeV2/src/mindeye/lib/python3.11/site-packages/kornia/augmentation/base.py:261\u001b[0m, in \u001b[0;36m_AugmentationBase.transform_inputs\u001b[0;34m(self, input, params, flags, transform, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m in_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_tensor(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m to_apply\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 261\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m to_apply\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    263\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_non_transform(in_tensor, params, flags, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "File \u001b[0;32m/fsx/proj-fmri/alexnguyen/mind-reader/MindEyeV2/src/mindeye/lib/python3.11/site-packages/kornia/augmentation/_2d/geometric/resize.py:71\u001b[0m, in \u001b[0;36mResize.apply_transform\u001b[0;34m(self, input, params, flags, transform)\u001b[0m\n\u001b[1;32m     66\u001b[0m     y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m][i, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     67\u001b[0m     y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m][i, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     68\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m resize(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28minput\u001b[39m[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, :, y1:y2, x1:x2],\n\u001b[1;32m     70\u001b[0m         out_size,\n\u001b[0;32m---> 71\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39m\u001b[43mflags\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m     72\u001b[0m         align_corners\u001b[38;5;241m=\u001b[39mflags[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malign_corners\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m flags[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresample\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [Resample\u001b[38;5;241m.\u001b[39mBILINEAR, Resample\u001b[38;5;241m.\u001b[39mBICUBIC]\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m         antialias\u001b[38;5;241m=\u001b[39mflags[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mantialias\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/lib/python3.11/enum.py:193\u001b[0m, in \u001b[0;36mproperty.__get__\u001b[0;34m(self, instance, ownerclass)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mproperty\u001b[39;00m(DynamicClassAttribute):\n\u001b[1;32m    185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    This is a descriptor, used to define attributes that act differently\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    when accessed through an enum member and through an enum class.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m    a corresponding enum member.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance, ownerclass\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "val_image, val_voxel = None, None\n",
    "test_image, test_voxel = None, None\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "utils.seed_everything(seed=args.seed, cudnn_deterministic=True)\n",
    "skip_train = True if epoch>=(num_epochs-1) else False # skip training if you are resuming from a fully trained model\n",
    "\n",
    "def pre_load_all_batches(epoch, num_iterations_per_epoch, dls, subj_list, images, voxels, batch_size, mixup_pct, num_epochs, data_type):\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    annot_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    for s, dl in enumerate(dls):\n",
    "        im_sizes = []\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            for iter, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(dl):    \n",
    "                image0 = images[behav0[:,0,0].cpu().long()].float()\n",
    "                # assert image0.shape[0] == 28, \"image0 shape is not 28\"\n",
    "                im_sizes.append(image0.shape[0])\n",
    "                image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "                \n",
    "                voxel0 = voxels[f'subj0{subj_list[s]}'][behav0[:,0,5].cpu().long()]\n",
    "                voxel0 = torch.Tensor(voxel0).to(data_type)\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                    perm_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = perm\n",
    "                    betas_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = betas\n",
    "                    select_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = select\n",
    "\n",
    "                voxel_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = voxel0\n",
    "\n",
    "                if iter >= num_iterations_per_epoch-1:\n",
    "                    break\n",
    "\n",
    "    return voxel_iters, image_iters, perm_iters, betas_iters, select_iters\n",
    "if args.when_to_preload == \"before_training\":\n",
    "    voxel_iters, image_iters, perm_iters, betas_iters, select_iters = pre_load_all_batches(0, num_iterations_per_epoch, train_dls, subj_list, images, voxels, batch_size, mixup_pct, num_epochs, data_type)\n",
    "    print (\"pre-loaded all batches for training\")\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    val_fwd_percent_correct = 0.\n",
    "    val_bwd_percent_correct = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    loss_prior_total = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "    val_loss_clip_total = 0.\n",
    "    val_loss_prior_total = 0.\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    if args.when_to_preload == \"before_each_epoch\":\n",
    "        voxel_iters, image_iters, perm_iters, betas_iters, select_iters = pre_load_all_batches(epoch, num_iterations_per_epoch, train_dls, subj_list, images, voxels, batch_size, mixup_pct, num_epochs, data_type)\n",
    "    \n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    if skip_train is False:\n",
    "        for train_i in np.random.permutation(num_iterations_per_epoch): # randomize the order of batches (important if we pre-load all batches)\n",
    "            with torch.cuda.amp.autocast(dtype=data_type):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                voxel_list = [voxel_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                image = image_iters[train_i].detach().to(device)\n",
    "\n",
    "                if use_image_aug: \n",
    "                    image = img_augment(image)\n",
    "\n",
    "                clip_target = utils.get_clip_embeddings(clip_model,image)\n",
    "                assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    perm_list = [perm_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                    perm = torch.cat(perm_list, dim=0)\n",
    "                    betas_list = [betas_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                    betas = torch.cat(betas_list, dim=0)\n",
    "                    select_list = [select_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                    select = torch.cat(select_list, dim=0)\n",
    "\n",
    "                voxel_ridge_list = [model.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "                voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "                clip_voxels, clip_voxels_proj = model.backbone(voxel_ridge)\n",
    "\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels_proj.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                if epoch < int(mixup_pct * num_epochs):                \n",
    "                    loss_clip = utils.mixco_nce(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006,\n",
    "                        perm=perm, betas=betas, select=select)\n",
    "                else:\n",
    "                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=epoch_temp)\n",
    "                    \n",
    "                loss_clip_total += loss_clip.item()\n",
    "                loss = loss_clip \n",
    "                print (\"loss_clip\", loss_clip.item())\n",
    "                \n",
    "                if use_prior:\n",
    "                    loss_prior, aligned_clip_voxels = model.diffusion_prior(text_embed=clip_voxels, image_embed=clip_target)\n",
    "                    aligned_clip_voxels /= model.diffusion_prior.image_embed_scale\n",
    "                    loss_prior_total += loss_prior.item()\n",
    "                    loss_prior *= prior_mult\n",
    "                    loss += loss_prior\n",
    "\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "                if lr_scheduler_type is not None:\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for val_i, (behav, past_behav, future_behav, old_behav) in enumerate(val_dl[f'subj0{subj_list[0]}']):\n",
    "                if val_image is None:\n",
    "                    voxel = voxels[f'subj0{subj_list[0]}'][behav[:,0,5].cpu().long()]\n",
    "                    image = behav[:,0,0].cpu().long()\n",
    "\n",
    "                    unique_image, sort_indices = torch.unique(image, return_inverse=True)\n",
    "                    for im in unique_image:\n",
    "                        locs = torch.where(im == image)[0]\n",
    "                        if val_image is None:\n",
    "                            val_image = images[im][None]\n",
    "                            val_voxel = torch.mean(voxel[locs],axis=0)[None]\n",
    "                        else:\n",
    "                            val_image = torch.vstack((val_image, images[im][None]))\n",
    "                            val_voxel = torch.vstack((val_voxel, torch.mean(voxel[locs],axis=0)[None]))\n",
    "\n",
    "                val_indices = torch.arange(len(val_voxel))[:300]\n",
    "                voxel = val_voxel[val_indices].to(device)\n",
    "                image = val_image[val_indices].to(device)\n",
    "                assert len(image) == 300\n",
    "\n",
    "                clip_target = utils.get_clip_embeddings(clip_model,image.float())\n",
    "\n",
    "                voxel_ridge = model.ridge(voxel,0)\n",
    "\n",
    "                clip_voxels, clip_voxels_proj = model.backbone(voxel_ridge)\n",
    "\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels_proj.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006)\n",
    "                \n",
    "                val_loss_clip_total += loss_clip.item()\n",
    "                loss = loss_clip\n",
    "\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device)\n",
    "                val_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()    \n",
    "                val_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "                utils.check_loss(loss)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "            for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):  \n",
    "                ## Average same-image repeats ##\n",
    "                if test_image is None:\n",
    "                    voxel = voxels[f'subj0{subj_list[0]}'][behav[:,0,5].cpu().long()]\n",
    "                    image = behav[:,0,0].cpu().long()\n",
    "\n",
    "                    unique_image, sort_indices = torch.unique(image, return_inverse=True)\n",
    "                    for im in unique_image:\n",
    "                        locs = torch.where(im == image)[0]\n",
    "                        if test_image is None:\n",
    "                            test_image = images[im][None]\n",
    "                            test_voxel = torch.mean(voxel[locs],axis=0)[None]\n",
    "                        else:\n",
    "                            test_image = torch.vstack((test_image, images[im][None]))\n",
    "                            test_voxel = torch.vstack((test_voxel, torch.mean(voxel[locs],axis=0)[None]))\n",
    "\n",
    "                test_indices = torch.arange(len(test_voxel))[:300]\n",
    "                voxel = test_voxel[test_indices].to(device)\n",
    "                image = test_image[test_indices].to(device)\n",
    "                assert len(image) == 300\n",
    "\n",
    "                clip_target = utils.get_clip_embeddings(clip_model,image.float())\n",
    "\n",
    "                voxel_ridge = model.ridge(voxel,0) # using ridge from 0th index of subj_list\n",
    "\n",
    "                clip_voxels, clip_voxels_proj = model.backbone(voxel_ridge)\n",
    "                \n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels_proj.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006)\n",
    "                \n",
    "                test_loss_clip_total += loss_clip.item()\n",
    "                loss = loss_clip\n",
    "                \n",
    "                if use_prior:\n",
    "                    loss_prior, aligned_clip_voxels = model.diffusion_prior(text_embed=clip_voxels, image_embed=clip_target)\n",
    "                    aligned_clip_voxels /= model.diffusion_prior.image_embed_scale\n",
    "                    test_loss_prior_total += loss_prior.item()\n",
    "                    loss_prior *= prior_mult\n",
    "                    loss += loss_prior\n",
    "\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "                break\n",
    "\n",
    "            # if utils.is_interactive(): clear_output(wait=True)\n",
    "            if skip_train: break\n",
    "            print(\"---\")\n",
    "\n",
    "            assert (test_i+1) == 1\n",
    "            logs = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"val/num_steps\": len(val_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"val/val_fwd_pct_correct\": val_fwd_percent_correct / (val_i + 1),\n",
    "                \"val/val_bwd_pct_correct\": val_bwd_percent_correct / (val_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"train/loss_prior_total\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior_total\": test_loss_prior_total / (test_i + 1),\n",
    "                }\n",
    "            \n",
    "            if use_prior: # output recons every ckpt\n",
    "                if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                    print(\"reconstructing...\")\n",
    "                    voxel_ridge = model.ridge(voxel[:1],0)\n",
    "                    clip_voxels, clip_voxels_proj = model.backbone(voxel_ridge)\n",
    "                    clip_model.to(torch.float32).to(\"cpu\")\n",
    "                    grid, _, _, _ = utils.reconstruction(\n",
    "                        image, voxel, clip_voxels.reshape(-1,257,768), clip_voxels_proj,\n",
    "                        clip_model,\n",
    "                        vd_pipe.image_unet, vd_pipe.vae, vd_pipe.scheduler,\n",
    "                        diffusion_priors = model.diffusion_prior,\n",
    "                        num_inference_steps = 20,\n",
    "                        n_samples_save = 1,\n",
    "                        guidance_scale = guidance_scale,\n",
    "                        timesteps_prior = timesteps,\n",
    "                        seed = seed,\n",
    "                        retrieve = False,\n",
    "                        plotting = True,\n",
    "                        img_variations = False,\n",
    "                        verbose = False,\n",
    "                    )\n",
    "                    clip_model.to(torch.device(f\"cuda:{local_rank}\")).to(data_type)\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/recons\"] = wandb.Image(grid, caption=f\"epoch{epoch:03d}\")\n",
    "                        plt.close()\n",
    "                    else:\n",
    "                        plt.show()\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            # Save model checkpoint and reconstruct\n",
    "            if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "                save_ckpt(f'last')\n",
    "\n",
    "            if wandb_log: wandb.log(logs)\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e87fde-815d-4452-9915-f5f5dacf7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mindeye)",
   "language": "python",
   "name": "mindeye"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
